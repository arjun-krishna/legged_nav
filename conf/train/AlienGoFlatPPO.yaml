seed: ${..seed}
runner_class_name: 'OnPolicyRunner'
policy:
  init_noise_std: 1.0
  actor_hidden_dims: [128, 64, 32]
  critic_hidden_dims: [128, 64, 32]
  activation: 'elu' # elu, relu, selu, crelu, lrelu, tanh, sigmoid

algorithm:
  value_loss_coef: 1.0
  use_clipped_value_loss: true
  clip_param: 0.2
  entropy_coef: 0.01
  num_learning_epochs: 5
  num_mini_batches: 4       # batch_size = num_envs * n_steps / num_mini_batches
  learning_rate: 1.e-3
  schedule: 'adaptive'
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.01
  max_grad_norm: 1.

runner:
  policy_class_name: 'ActorCritic'
  algorithm_class_name: 'PPO'
  num_steps_per_env: 24 # per iteration
  max_iterations: ${resolve_default:300,${...max_iterations}}

  # logging
  save_interval: 50
  experiment_name: ${resolve_default:AlienGoFlat,${...experiment_name}}
  run_name: ${resolve_default:'',${...run_name}}

  # load and resume
  resume: ${...resume}
  load_run: ${...run}
  load_checkpoint: ${...checkpoint}
  resume_path: null
