name: AlienGoNavFlat
seed: ${..seed}
base_type: nav

env:
  num_envs: ${resolve_default:384,${...num_envs}}
  num_observations: 63 # 3 (60 lidar dist)
  num_ll_observations: 48
  num_privileged_obs: null
  num_actions: 2
  num_ll_actions: 12
  env_spacing: 3
  send_timeouts: true
  episode_length_s: 20
  debug_viz: ${resolve_default:False,${...test}}

obstacle:
  static:
    num: 10
    height: [1.0, 2.0]
    width: [0.5, 0.5]
    depth: [0.5, 0.5]
    spawn_range: [1.0, 5.0]
  dynamic:
    num: 0
  magic_spawn:
    num: 0

lidar:
  use_lidar: true
  num_reflections: 60   # [int] rays to shoot
  fov: 360              # [deg]
  max_laser_dist: 5.0   # [m]

terrain:
  mesh_type: 'plane'
  horizontal_scale: 0.1 # [m]
  vertical_scale: 0.005 # [m]
  border_size: 25       # [m]
  curriculum: true
  static_friction: 1.0
  dynamic_friction: 1.0
  restitution: 0.

  # rough terrains
  measure_heights: false
  measured_points_x: ${arange:-0.8,0.8,0.1}
  measured_points_y: ${arange:-0.5,0.5,0.1}
  selected: False  # select unique terrain type and pass all arguments
  terrain_kwargs: null  # arguments for selected terrain
  max_init_terrain_level: 5 # starting curriculum state
  terrain_length: 8.
  terrain_width: 8.
  num_rows: 10  # terrain rows (levels)
  num_cols: 20  # terrain cols (types)
  # terrain types: [smooth slopes, rough slope, stairs up, stairs down, discrete]
  terrain_proportions: [0.1, 0.1, 0.35, 0.25, 0.2]
  slope_threshold: 0.75 # slopes above this are converted to vertical surfaces

commands:
  curriculum: False
  max_curriculum: 1.
  num_commands: 2 # default: goal_x, goal_y
  resampling_time: 20 # time before command is changed[s]
  ranges:
    goal_x: [2., 7.] #[-7., 7.]    # [m]
    goal_y: [-0., 0.] #[-7., 7.]    # [m]
  loco_net: "{LEGGED_GYM_ROOT_DIR}/resources/loco_nets/aliengo_plane.pt"

init_state:
  pos: [0., 0., 0.38] # x,y,z [m]
  rot: [0., 0., 0., 1.] # x,y,z,w [quat]
  lin_vel: [0., 0., 0.] # x,y,z [m/s]
  ang_vel: [0., 0., 0.] # x,y,z [rad/s]
  default_joint_angles:
    FL_hip_joint: 0.1
    RL_hip_joint: 0.1
    FR_hip_joint: -0.1
    RR_hip_joint: -0.1

    FL_thigh_joint: 0.8
    RL_thigh_joint: 1.0
    FR_thigh_joint: 0.8
    RR_thigh_joint: 1.0

    FL_calf_joint: -1.5
    RL_calf_joint: -1.5
    FR_calf_joint: -1.5
    RR_calf_joint: -1.5

control:
  control_type: 'P' # P: position, V: velocity, T: torques
  # PD control params per dof
  stiffness: 
    joint: 40.    # [N*m/rad]
  damping:
    joint: 2.    # [N*m*s/rad]
  # action scale: (target = action_scale * action + default_dof_pos)
  action_scale: 0.25
  # decimation: number of control actions updates @ sim dt per policy dt
  decimation: 4

asset:
  file: "{LEGGED_GYM_ROOT_DIR}/resources/robots/aliengo/urdf/aliengo.urdf"
  foot_name: "foot"
  penalize_contacts_on: ["thigh", "calf"]
  terminate_after_contacts_on: ["base", "trunk", "hip"]
  disable_gravity: False
  collapse_fixed_joints: true
  fix_base_link: False
  default_dof_drive_mode: 3 # 0: none, 1: pos, 1: vel, 3: effort
  self_collisions: 0 # 1: disable, 0: enable
  replace_cylinder_with_capsule: true
  flip_visual_attachments: true

  density: 0.001
  angular_damping: 0.
  linear_damping: 0.
  max_angular_velocity: 1000.
  max_linear_velocity: 1000.
  armature: 0.
  thickness: 0.01

domain_rand:
  randomize_friction: true
  friction_range: [0., 1.5]
  randomize_base_mass: false
  added_mass_range: [-1., 1.]
  push_robots: true
  push_interval_s: 15
  max_push_vel_xy: 1.

rewards:
  scales:
    termination: -200.0 # collision results in termination
    lin_vel_z: -0.0 #-2.0
    ang_vel_xy: -0.0 #-0.05
    orientation: -0.0 #-5.0
    torques: -0.0 #-0.000025
    dof_vel: -0.0
    dof_acc: 0.0 #!!float -2.5e-7
    base_height: -0.0
    feet_air_time: 0.0 #2.0
    collision: -5.0
    feet_stumble: -0.0
    action_rate: -0.0 #-0.01
    dof_pos_limits: 0.0 #-10.0
    progress: 10.0
    success: 1000.0
    relative_yaw: -5.0
  
  only_positive_rewards: true # negative total rewards are clipped at zero (avoid early termination problem)
  dist_threshold: 0.5
  tracking_sigma: 0.25 # tracking reward = exp(-error^2/sigma)
  soft_dof_pos_limit: 0.9
  soft_dof_vel_limit: 1.
  soft_torque_limit: 1.
  base_height_target: 0.5
  max_contact_force: 350. # forces above this value are penalized

normalization:
  obs_scale:
    pos: 1.0
    lin_vel: 2.0
    ang_vel: 0.25
    dof_pos: 1.0
    dof_vel: 0.05
    height_measurements: 5.0
  clip_observations: 100.
  clip_actions: 100.

noise:
  add_noise: true
  noise_level: 1. # scale other values
  noise_scales:
    pos: 0.05
    dof_pos: 0.01
    dof_vel: 1.5
    lin_vel: 0.1
    ang_vel: 0.2
    gravity: 0.05
    height_measurements: 0.1
    lidar_measurements: 0.1

viewer:
  ref_env: 0
  pos: [10, 0, 6]    # [m]
  lookat: [11, 5, 3] # [m]

sim:
  dt: 0.005
  substep: 1
  gravity: [0., 0., -9.81]  # [m/s^2]
  up_axis: 1  # 0: y, 1: z

  physx:
    num_threads: ${....num_threads}
    solver_type: ${....solver_type}
    num_subscenes: ${....subscenes}
    use_gpu: ${contains:"cuda",${....sim_device}}
    num_position_iterations: 4
    num_velocity_iterations: 0
    contact_offset: 0.01  # [m]
    rest_offset: 0.0   # [m]
    bounce_threshold_velocity: 0.5 #0.5 [m/s]
    max_depenetration_velocity: 1.0
    max_gpu_contact_pairs: 8388608 #2**23 | 2**24 -> needed for 8000 envs and more
    default_buffer_size_multiplier: 5
    contact_collection: 2 # 0: never, 1: last sub-step, 2: all sub-steps (default=2)
